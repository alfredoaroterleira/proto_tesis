link = "https://raw.githubusercontent.com/alfredoaroterleira/UNMSM_Soto/refs/heads/main/data.csv"
data = read.csv(link)
data
library (dplyr)
library(tidyverse)
data = data %>%
select(p1, p4, p6, p8, p9, p10, p13, p14, p16, p18)
#data = data %>%
#mutate(p16 = ifelse(p16 == 0, NA, p16))
str(data)
# Verificamos la
glimpse(data)
data <- data.frame(lapply(data, as.numeric))
str(data)
#creamos la matriz de correlaciones (en nuestro caso policóricas, ya que tenemos variables ordinales)
library(polycor)
library(psych)
cor_matrix = polychoric(data, correct = 0)$rho
round(cor_matrix,2)
library(ggcorrplot)
ggcorrplot(cor_matrix)
library(psych)
psych::KMO(cor_matrix)  #mayor a 0.6 es aceptable
library(psych)
psych::KMO(cor_matrix)  #mayor a 0.6 es aceptable
cortest.bartlett(cor_matrix,n=nrow(data))$p.value>0.05
library(matrixcalc)
is.singular.matrix(cor_matrix)
eigen_values <- eigen(cor_matrix)$values
print(eigen_values)
#Gráfico de sedimentación
library(psych)
psych::scree(cor_matrix, factors = TRUE, main = "Scree Plot")
#Análisis paralelo
library(psych)
psych::fa.parallel(cor_matrix,
n.obs = nrow(data),
fa = "fa",
main = "Análisis Paralelo")
library(GPArotation)
afe <- psych::fa(
r = cor_matrix,          # Matriz de correlaciones
nfactors = 2,            # Número de factores
n.obs = nrow(data), # Número de observaciones
rotate = "promax",       # Rotación oblicua (factores correlacionados)
fm = "ml",               # Método: Máxima Verosimilitud
scores = "regression",   # Método para calcular puntuaciones
missing = TRUE
)
#Calculamos puntuaciones
puntuaciones <- factor.scores(
data,                  # Datos originales
afe,                   # Objeto del análisis factorial
method = "regression", # Método de estimación
rho = cor_matrix       # Matriz de correlaciones usada
)
link = "https://raw.githubusercontent.com/alfredoaroterleira/UNMSM_Soto/refs/heads/main/data.csv"
data = read.csv(link)
data
library (dplyr)
library(tidyverse)
data = data %>%
select(p1, p4, p6, p8, p9, p10, p13, p14, p16)
#data = data %>%
#mutate(p16 = ifelse(p16 == 0, NA, p16))
str(data)
link = "https://raw.githubusercontent.com/alfredoaroterleira/UNMSM_Soto/refs/heads/main/data.csv"
data = read.csv(link)
data
library (dplyr)
library(tidyverse)
data = data %>%
select(p1, p4, p6, p8, p9, p10, p13, p14, p18)
#data = data %>%
#mutate(p16 = ifelse(p16 == 0, NA, p16))
str(data)
# Verificamos la
glimpse(data)
data <- data.frame(lapply(data, as.numeric))
str(data)
#creamos la matriz de correlaciones (en nuestro caso policóricas, ya que tenemos variables ordinales)
library(polycor)
library(psych)
cor_matrix = polychoric(data, correct = 0)$rho
round(cor_matrix,2)
library(ggcorrplot)
ggcorrplot(cor_matrix)
library(psych)
psych::KMO(cor_matrix)  #mayor a 0.6 es aceptable
cortest.bartlett(cor_matrix,n=nrow(data))$p.value>0.05
library(matrixcalc)
is.singular.matrix(cor_matrix)
eigen_values <- eigen(cor_matrix)$values
print(eigen_values)
#Gráfico de sedimentación
library(psych)
psych::scree(cor_matrix, factors = TRUE, main = "Scree Plot")
#Análisis paralelo
library(psych)
psych::fa.parallel(cor_matrix,
n.obs = nrow(data),
fa = "fa",
main = "Análisis Paralelo")
library(GPArotation)
afe <- psych::fa(
r = cor_matrix,          # Matriz de correlaciones
nfactors = 2,            # Número de factores
n.obs = nrow(data), # Número de observaciones
rotate = "promax",       # Rotación oblicua (factores correlacionados)
fm = "ml",               # Método: Máxima Verosimilitud
scores = "regression",   # Método para calcular puntuaciones
missing = TRUE
)
#Calculamos puntuaciones
puntuaciones <- factor.scores(
data,                  # Datos originales
afe,                   # Objeto del análisis factorial
method = "regression", # Método de estimación
rho = cor_matrix       # Matriz de correlaciones usada
)
link = "https://raw.githubusercontent.com/alfredoaroterleira/UNMSM_Soto/refs/heads/main/data.csv"
data = read.csv(link)
data
link = "https://raw.githubusercontent.com/alfredoaroterleira/UNMSM_Soto/refs/heads/main/data.csv"
data = read.csv(link)
data
library (dplyr)
library(tidyverse)
data = data %>%
select(p1, p4, p6, p8, p9, p10, p13, p14, p18)
data <- data.frame(lapply(data, as.numeric))
str(data)
# Verificamos la
glimpse(data)
library(polycor)
corMatrix=polycor::hetcor(data)$correlations
link = "https://raw.githubusercontent.com/alfredoaroterleira/UNMSM_Soto/refs/heads/main/data.csv"
data = read.csv(link)
data
library (dplyr)
library(tidyverse)
data = data %>%
select(p1, p4, p6, p8, p9, p10, p13, p14, p18)
#data = data %>%
#mutate(p16 = ifelse(p16 == 0, NA, p16))
str(data)
# Verificamos la
glimpse(data)
data <- data.frame(lapply(data, as.numeric))
str(data)
#creamos la matriz de correlaciones (en nuestro caso policóricas, ya que tenemos variables ordinales)
library(polycor)
library(psych)
cor_matrix = polychoric(data, correct = 0)$rho
round(cor_matrix,2)
library(ggcorrplot)
ggcorrplot(cor_matrix)
library(psych)
psych::KMO(cor_matrix)  #mayor a 0.6 es aceptable
cortest.bartlett(cor_matrix,n=nrow(data))$p.value>0.05
library(matrixcalc)
is.singular.matrix(cor_matrix)
eigen_values <- eigen(cor_matrix)$values
print(eigen_values)
#Gráfico de sedimentación
library(psych)
psych::scree(cor_matrix, factors = TRUE, main = "Scree Plot")
#Análisis paralelo
library(psych)
psych::fa.parallel(cor_matrix,
n.obs = nrow(data),
fa = "fa",
main = "Análisis Paralelo")
library(GPArotation)
afe <- psych::fa(
r = cor_matrix,          # Matriz de correlaciones
nfactors = 2,            # Número de factores
n.obs = nrow(data), # Número de observaciones
rotate = "promax",       # Rotación oblicua (factores correlacionados)
fm = "ml",               # Método: Máxima Verosimilitud
scores = "regression",   # Método para calcular puntuaciones
missing = TRUE
)
# A. Cargas factoriales
print("Cargas Factoriales:")
print(afe$loadings, cutoff = 0.40, sort = TRUE)
# B. Comunalidades
print("Comunalidades:")
print(afe$communalities)
# C. Varianza explicada
print("Varianza Explicada:")
print(afe$Vaccounted)
# D. ¿Qué variables contribuyen a la construcción de más de un factor?
sort(afe$complexity)
# E. Tucker Lewis
afe$TLI
# F. RMS cerca a cero
afe$rms
# F. RMS cerca a cero
afe$rms
# G. RMSEA cerca a cero
afe$RMSEA
# H. BIC
afe$BIC
# I. Diagrama de factores
library(psych)
psych::fa.diagram(afe, main = "Diagrama de Factores")
link = "https://raw.githubusercontent.com/alfredoaroterleira/UNMSM_Soto/refs/heads/main/data.csv"
data = read.csv(link)
data
library (dplyr)
library(tidyverse)
data = data %>%
select(p1, p4, p6, p8, p9, p10, p13, p14, p16, p18)
data = data %>%
mutate(p16 = ifelse(p16 == 0, NA, p16))
str(data)
# Verificamos la
glimpse(data)
data <- data.frame(lapply(data, as.numeric))
str(data)
#creamos la matriz de correlaciones (en nuestro caso policóricas, ya que tenemos variables ordinales)
library(polycor)
library(psych)
cor_matrix = polychoric(data, correct = 0)$rho
round(cor_matrix,2)
library(ggcorrplot)
ggcorrplot(cor_matrix)
library(psych)
psych::KMO(cor_matrix)  #mayor a 0.6 es aceptable
cortest.bartlett(cor_matrix,n=nrow(data))$p.value>0.05
library(matrixcalc)
is.singular.matrix(cor_matrix)
eigen_values <- eigen(cor_matrix)$values
print(eigen_values)
#Gráfico de sedimentación
library(psych)
psych::scree(cor_matrix, factors = TRUE, main = "Scree Plot")
#Análisis paralelo
library(psych)
psych::fa.parallel(cor_matrix,
n.obs = nrow(data),
fa = "fa",
main = "Análisis Paralelo")
library(GPArotation)
afe <- psych::fa(
r = cor_matrix,          # Matriz de correlaciones
nfactors = 2,            # Número de factores
n.obs = nrow(data), # Número de observaciones
rotate = "promax",       # Rotación oblicua (factores correlacionados)
fm = "ml",               # Método: Máxima Verosimilitud
scores = "regression",   # Método para calcular puntuaciones
missing = TRUE
)
# A. Cargas factoriales
print("Cargas Factoriales:")
print(afe$loadings, cutoff = 0.40, sort = TRUE)
# B. Comunalidades
print("Comunalidades:")
print(afe$communalities)
# C. Varianza explicada
print("Varianza Explicada:")
print(afe$Vaccounted)
# D. ¿Qué variables contribuyen a la construcción de más de un factor?
sort(afe$complexity)
# E. Tucker Lewis
afe$TLI
# F. RMS cerca a cero
afe$rms
# G. RMSEA cerca a cero
afe$RMSEA
# H. BIC
afe$BIC
# I. Diagrama de factores
library(psych)
psych::fa.diagram(afe, main = "Diagrama de Factores")
library(dplyr)
library(psych)
data <- data %>%
mutate(
indice_compromiso = rowMeans(select(., p9, p14, p8, p18, p16), na.rm = TRUE),
indice_compromiso_z = scale(indice_compromiso)
)
data <- data %>%
mutate(
indice_participacion = rowMeans(select(., p10, p13), na.rm = TRUE),
indice_participacion_z = scale(indice_participacion)
)
#Verificamos
psych::describe(data %>% select(stars_with("indice")))
library(dplyr)
library(psych)
data <- data %>%
mutate(
indice_compromiso = rowMeans(select(., p9, p14, p8, p18, p16), na.rm = TRUE),
indice_compromiso_z = scale(indice_compromiso)
)
data <- data %>%
mutate(
indice_participacion = rowMeans(select(., p10, p13), na.rm = TRUE),
indice_participacion_z = scale(indice_participacion)
)
#Verificamos
psych::describe(data %>% select(starts_with("indice")))
link = "https://raw.githubusercontent.com/alfredoaroterleira/UNMSM_Soto/refs/heads/main/data.csv"
data = read.csv(link)
data
library (dplyr)
library(tidyverse)
data = data %>%
select(p1, p4, p6, p8, p9, p10, p13, p14, p16, p18)
data = data %>%
mutate(p16 = ifelse(p16 == 0, NA, p16))
str(data)
# Verificamos la
glimpse(data)
data <- data.frame(lapply(data, as.numeric))
str(data)
#creamos la matriz de correlaciones (en nuestro caso policóricas, ya que tenemos variables ordinales)
library(polycor)
library(psych)
cor_matrix = polychoric(data, correct = 0)$rho
round(cor_matrix,2)
library(ggcorrplot)
ggcorrplot(cor_matrix)
library(psych)
psych::KMO(cor_matrix)  #mayor a 0.6 es aceptable
cortest.bartlett(cor_matrix,n=nrow(data))$p.value>0.05
library(matrixcalc)
is.singular.matrix(cor_matrix)
eigen_values <- eigen(cor_matrix)$values
print(eigen_values)
#Gráfico de sedimentación
library(psych)
psych::scree(cor_matrix, factors = TRUE, main = "Scree Plot")
#Análisis paralelo
library(psych)
psych::fa.parallel(cor_matrix,
n.obs = nrow(data),
fa = "fa",
main = "Análisis Paralelo")
library(GPArotation)
afe <- psych::fa(
r = cor_matrix,          # Matriz de correlaciones
nfactors = 2,            # Número de factores
n.obs = nrow(data), # Número de observaciones
rotate = "promax",       # Rotación oblicua (factores correlacionados)
fm = "ml",               # Método: Máxima Verosimilitud
scores = "regression",   # Método para calcular puntuaciones
missing = TRUE
)
# A. Cargas factoriales
print("Cargas Factoriales:")
print(afe$loadings, cutoff = 0.40, sort = TRUE)
# B. Comunalidades
print("Comunalidades:")
print(afe$communalities)
# C. Varianza explicada
print("Varianza Explicada:")
print(afe$Vaccounted)
# D. ¿Qué variables contribuyen a la construcción de más de un factor?
sort(afe$complexity)
# E. Tucker Lewis
afe$TLI
# F. RMS cerca a cero
afe$rms
# G. RMSEA cerca a cero
afe$RMSEA
# H. BIC
afe$BIC
# I. Diagrama de factores
library(psych)
psych::fa.diagram(afe, main = "Diagrama de Factores")
library(dplyr)
library(psych)
data <- data %>%
mutate(
indice_compromiso = rowMeans(select(., p9, p14, p8, p18, p16), na.rm = TRUE),
indice_compromiso_z = scale(indice_compromiso)
)
data <- data %>%
mutate(
indice_participacion = rowMeans(select(., p10, p13), na.rm = TRUE),
indice_participacion_z = scale(indice_participacion)
)
#Verificamos
psych::describe(data %>% select(starts_with("indice")))
View(data)
View(data)
contratos = read.csv("https://raw.githubusercontent.com/alfredoaroterleira/proto_tesis/refs/heads/main/data_expo.csv")
head(contratos)
library(dplyr)
contratos = contratos %>%
select("departamento", "empresa_1", "empresa_2", "empresa3", "empresa_4", "empresa_5")
library(dplyr)
contratos = contratos %>%
select("departamento", "empresa_1", "empresa_2", "empresa_3", "empresa_4", "empresa_5")
str(contratos)
contratos$empresa_1 = as.character(contratos$empresa_1)
contratos$empresa_2 = as.character(contratos$empresa_2)
contratos$empresa_3 = as.character(contratos$empresa_3)
contratos$empresa_4 = as.character(contratos$empresa_4)
contratos$empresa_5 = as.character(contratos$empresa_5)
str(contratos)
#lo pasamos a formato long
library(tidyverse)
# Supongamos que tu data se llama 'contratos'
contratos_long <- contratos %>%
pivot_longer(cols = starts_with("empresa"),
names_to = "empresa_num",
values_to = "empresa") %>%
filter(!is.na(empresa))  # eliminar filas vacías
head(contratos_long)
#creamos red bipartita
library(igraph)
# Crear red bipartita
g_bipartita <- graph_from_data_frame(contratos_long, directed = FALSE)
# Indicar tipo de nodo (empresa = TRUE, distrito = FALSE)
V(g_bipartita)$type <- !(V(g_bipartita)$name %in% contratos$district)
#creamos red bipartita
# Peso = cantidad de contratos que la empresa tiene con el distrito
red_bipartita <- contratos_long %>%
count(departamento, empresa, name = "peso")
# Guardar como CSV
write.csv(red_bipartita, "red_bipartita_empresa_distrito.csv", row.names = FALSE)
View(red_bipartita)
# Obtener todas las combinaciones de empresas por contrato
red_unipartita <- contratos_long %>%
group_by(departamento) %>%
summarise(pares = combn(empresa, 2, simplify = FALSE)) %>%
unnest(pares) %>%
mutate(source = map_chr(pares, 1),
target = map_chr(pares, 2)) %>%
select(source, target)
library(tidyverse)
# Partimos de contratos_long, que ya está en formato largo
# (columnas: distrito, empresa)
# 1. Agrupar por contrato (distrito) y crear combinaciones solo si hay al menos 2 empresas
red_unipartita <- contratos_long %>%
group_by(distrito) %>%
summarise(pares = list({
empresas <- unique(empresa)
if (length(empresas) >= 2) combn(empresas, 2, simplify = FALSE) else list()
}), .groups = "drop") %>%
unnest(pares) %>%
mutate(source = map_chr(pares, 1),
target = map_chr(pares, 2)) %>%
select(source, target)
library(tidyverse)
# Partimos de contratos_long, que ya está en formato largo
# (columnas: distrito, empresa)
# 1. Agrupar por contrato (distrito) y crear combinaciones solo si hay al menos 2 empresas
red_unipartita <- contratos_long %>%
group_by(departamento) %>%
summarise(pares = list({
empresas <- unique(empresa)
if (length(empresas) >= 2) combn(empresas, 2, simplify = FALSE) else list()
}), .groups = "drop") %>%
unnest(pares) %>%
mutate(source = map_chr(pares, 1),
target = map_chr(pares, 2)) %>%
select(source, target)
View(red_unipartita)
red_unipartita_pesada <- red_unipartita %>%
count(source, target, name = "peso")
write.csv(red_unipartita_pesada, "red_unipartita_empresas.csv", row.names = FALSE)
View(red_unipartita_pesada)
setwd("C:/Users/USUARIO/Documents/GitHub/proto_tesis")
contratos = read.csv("https://raw.githubusercontent.com/alfredoaroterleira/proto_tesis/refs/heads/main/data_expo.csv")
head(contratos)
library(dplyr)
contratos = contratos %>%
select("departamento", "empresa_1", "empresa_2", "empresa_3", "empresa_4", "empresa_5")
str(contratos)
contratos$empresa_1 = as.character(contratos$empresa_1)
contratos$empresa_2 = as.character(contratos$empresa_2)
contratos$empresa_3 = as.character(contratos$empresa_3)
contratos$empresa_4 = as.character(contratos$empresa_4)
contratos$empresa_5 = as.character(contratos$empresa_5)
str(contratos)
#lo pasamos a formato long
library(tidyverse)
# Supongamos que tu data se llama 'contratos'
contratos_long <- contratos %>%
pivot_longer(cols = starts_with("empresa"),
names_to = "empresa_num",
values_to = "empresa") %>%
filter(!is.na(empresa))  # eliminar filas vacías
head(contratos_long)
#creamos red bipartita
# Peso = cantidad de contratos que la empresa tiene con el distrito
red_bipartita <- contratos_long %>%
count(departamento, empresa, name = "peso")
# Guardar como CSV
write.csv(red_bipartita, "red_bipartita_empresa_distrito.csv", row.names = FALSE)
library(tidyverse)
# Partimos de contratos_long, que ya está en formato largo
# (columnas: distrito, empresa)
# 1. Agrupar por contrato (distrito) y crear combinaciones solo si hay al menos 2 empresas
red_unipartita <- contratos_long %>%
group_by(departamento) %>%
summarise(pares = list({
empresas <- unique(empresa)
if (length(empresas) >= 2) combn(empresas, 2, simplify = FALSE) else list()
}), .groups = "drop") %>%
unnest(pares) %>%
mutate(source = map_chr(pares, 1),
target = map_chr(pares, 2)) %>%
select(source, target)
red_unipartita_pesada <- red_unipartita %>%
count(source, target, name = "peso")
write.csv(red_unipartita_pesada, "red_unipartita_empresas.csv", row.names = FALSE)
